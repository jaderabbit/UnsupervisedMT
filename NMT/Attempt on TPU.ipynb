{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attempt on TPU.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "22pfJIrdzVkm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unsupervised NMT\n",
        "\n",
        "## Installation"
      ]
    },
    {
      "metadata": {
        "id": "ikNFFGD2a1Ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "1e071e18-8e07-4b51-f890-7c1e7121a410"
      },
      "cell_type": "code",
      "source": [
        "# Mount Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYhy9Vx3MQCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "015d1b7f-326d-4743-8fba-37f41d492765"
      },
      "cell_type": "code",
      "source": [
        "# Install pytorch using wheel\n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 33kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x590a4000 @  0x7fc42c3182a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D9o5IdtMed3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "f1940570-6bb9-4ea4-ac2b-043f806d6cf5"
      },
      "cell_type": "code",
      "source": [
        "# Install the github repo\n",
        "!git clone https://github.com/jaderabbit/UnsupervisedMT.git\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UnsupervisedMT'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 122 (delta 32), reused 29 (delta 17), pack-reused 71\u001b[K\n",
            "Receiving objects: 100% (122/122), 112.33 KiB | 2.44 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rx7Wnyp4s4Oe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "093ebc56-b95a-4fad-d4dc-b328650ae932"
      },
      "cell_type": "code",
      "source": [
        "#!cd UnsupervisedMT; git pull origin master"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)   \u001b[K\rremote: Counting objects:  14% (2/14)   \u001b[K\rremote: Counting objects:  21% (3/14)   \u001b[K\rremote: Counting objects:  28% (4/14)   \u001b[K\rremote: Counting objects:  35% (5/14)   \u001b[K\rremote: Counting objects:  42% (6/14)   \u001b[K\rremote: Counting objects:  50% (7/14)   \u001b[K\rremote: Counting objects:  57% (8/14)   \u001b[K\rremote: Counting objects:  64% (9/14)   \u001b[K\rremote: Counting objects:  71% (10/14)   \u001b[K\rremote: Counting objects:  78% (11/14)   \u001b[K\rremote: Counting objects:  85% (12/14)   \u001b[K\rremote: Counting objects:  92% (13/14)   \u001b[K\rremote: Counting objects: 100% (14/14)   \u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)   \u001b[K\rremote: Compressing objects: 100% (2/2)   \u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  12% (1/8)   \rUnpacking objects:  25% (2/8)   \rUnpacking objects:  37% (3/8)   \rUnpacking objects:  50% (4/8)   \rUnpacking objects:  62% (5/8)   \rUnpacking objects:  75% (6/8)   \rUnpacking objects:  87% (7/8)   \rUnpacking objects: 100% (8/8)   \rUnpacking objects: 100% (8/8), done.\n",
            "From https://github.com/jaderabbit/UnsupervisedMT\n",
            " * branch            master     -> FETCH_HEAD\n",
            "   c51738b..12112a4  master     -> origin/master\n",
            "Updating c51738b..12112a4\n",
            "Fast-forward\n",
            " NMT/src/utils.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ajea09iRzhMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## English-Zulu NMT\n",
        "\n",
        "### Generate Data"
      ]
    },
    {
      "metadata": {
        "id": "HOad2OKLewzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5509
        },
        "outputId": "b1d66ad4-b960-41ef-eef2-0ac4f81ba38a"
      },
      "cell_type": "code",
      "source": [
        "# cd into github repo\n",
        "# Run preparation\n",
        "!cd UnsupervisedMT/NMT; ./get_my_data.sh zu https://github.com/LauraMartinus/ukuxhumana/raw/master/leipzig/web_2013_100K_mono.zu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===== Input parameters\n",
            " source: en\n",
            " target: zu\n",
            " target monolingual url: https://github.com/LauraMartinus/ukuxhumana/raw/master/leipzig/web_2013_100K_mono.zu\n",
            " \n",
            "Cloning Moses from GitHub repository...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 147198, done.\u001b[K\n",
            "remote: Total 147198 (delta 0), reused 0 (delta 0), pack-reused 147198\u001b[K\n",
            "Receiving objects: 100% (147198/147198), 129.67 MiB | 18.82 MiB/s, done.\n",
            "Resolving deltas: 100% (113765/113765), done.\n",
            "Moses found in: /content/UnsupervisedMT/NMT/tools/mosesdecoder\n",
            "Cloning fastBPE from GitHub repository...\n",
            "Cloning into 'fastBPE'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Total 24 (delta 0), reused 0 (delta 0), pack-reused 24\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "fastBPE found in: /content/UnsupervisedMT/NMT/tools/fastBPE\n",
            "Compiling fastBPE...\n",
            "fastBPE compiled in: /content/UnsupervisedMT/NMT/tools/fastBPE/fast\n",
            "Cloning fastText from GitHub repository...\n",
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 2729 (delta 42), reused 52 (delta 23), pack-reused 2638\u001b[K\n",
            "Receiving objects: 100% (2729/2729), 7.72 MiB | 24.33 MiB/s, done.\n",
            "Resolving deltas: 100% (1687/1687), done.\n",
            "fastText found in: /content/UnsupervisedMT/NMT/tools/fastText\n",
            "Compiling fastText...\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/args.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/dictionary.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/productquantizer.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/matrix.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/qmatrix.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/vector.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/model.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/utils.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/meter.cc\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops -c src/fasttext.cc\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid fasttext::FastText::printLabelStats(std::istream&, int32_t, fasttext::real) const\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/fasttext.cc:445:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid fasttext::FastText::writePerLabelMetrics(std::ostream&, fasttext::Meter&) const\u001b[m\u001b[K’ is deprecated: This function is deprecated and will be removed along with `printLabelStats`. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   writePerLabelMetrics(std::cout, meter\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ksrc/fasttext.cc:10:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Ksrc/fasttext.h:126:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   void \u001b[01;36m\u001b[KwritePerLabelMetrics\u001b[m\u001b[K(std::ostream&, Meter&) const;\n",
            "        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "c++ -pthread -std=c++0x -march=native -O3 -funroll-loops args.o dictionary.o productquantizer.o matrix.o qmatrix.o vector.o model.o utils.o meter.o fasttext.o src/main.cc -o fasttext\n",
            "\u001b[01m\u001b[Ksrc/main.cc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test(const std::vector<std::__cxx11::basic_string<char> >&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/main.cc:159:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid fasttext::FastText::writePerLabelMetrics(std::ostream&, fasttext::Meter&) const\u001b[m\u001b[K’ is deprecated: This function is deprecated and will be removed along with `printLabelStats`. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     fasttext.writePerLabelMetrics(std::cout, meter\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ksrc/main.cc:15:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Ksrc/fasttext.h:126:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   void \u001b[01;36m\u001b[KwritePerLabelMetrics\u001b[m\u001b[K(std::ostream&, Meter&) const;\n",
            "        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "fastText compiled in: /content/UnsupervisedMT/NMT/tools/fastText/fasttext\n",
            "Downloading English files...\n",
            "--2018-11-21 19:43:41--  http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2007.en.shuffled.gz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206963851 (197M) [application/x-gzip]\n",
            "Saving to: ‘news.2007.en.shuffled.gz’\n",
            "\n",
            "news.2007.en.shuffl 100%[===================>] 197.38M  2.29MB/s    in 80s     \n",
            "\n",
            "2018-11-21 19:45:02 (2.47 MB/s) - ‘news.2007.en.shuffled.gz’ saved [206963851/206963851]\n",
            "\n",
            "Downloading zu files...\n",
            "--2018-11-21 19:45:02--  https://github.com/LauraMartinus/ukuxhumana/raw/master/leipzig/web_2013_100K_mono.zu\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/leipzig/web_2013_100K_mono.zu [following]\n",
            "--2018-11-21 19:45:02--  https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/leipzig/web_2013_100K_mono.zu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11324613 (11M) [text/plain]\n",
            "Saving to: ‘mono.zu’\n",
            "\n",
            "mono.zu             100%[===================>]  10.80M  20.0MB/s    in 0.5s    \n",
            "\n",
            "2018-11-21 19:45:03 (20.0 MB/s) - ‘mono.zu’ saved [11324613/11324613]\n",
            "\n",
            "Decompressing news.2007.en.shuffled.gz...\n",
            "ERROR: Number of lines doesn't match! Be sure you have 100000 sentences in your EN monolingual data.\n",
            "Truncating file...\n",
            "Tokenize monolingual data...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 48\n",
            "Tokenizer Version 1.1\n",
            "Language: zu\n",
            "Number of threads: 48\n",
            "WARNING: No known abbreviations for language 'zu', attempting fall-back to English version...\n",
            "EN monolingual data tokenized in: /content/UnsupervisedMT/NMT/data/mono/mono.en.tok\n",
            "zu monolingual data tokenized in: /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok\n",
            "Learning BPE codes...\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.en.tok ...\n",
            "Read 2363848 words (100949 unique) from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok ...\n",
            "Read 1557623 words (349064 unique) from text file.\n",
            "tcmalloc: large alloc 12000002048 bytes == 0x563b49d70000 @  0x7f4c155db887 0x563b4007c6fd 0x563b400717ef 0x7f4c14a16b97 0x563b40071a1a\n",
            "BPE learned in /content/UnsupervisedMT/NMT/data/mono/bpe_codes\n",
            "Applying BPE codes...\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.en.tok ...\n",
            "Read 2363848 words (100949 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/mono/mono.en.tok ...\n",
            "Modified 2363848 words from text file.\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok ...\n",
            "Read 1557623 words (254822 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok ...\n",
            "Modified 1557623 words from text file.\n",
            "BPE codes applied to EN in: /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000\n",
            "BPE codes applied to zu in: /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000\n",
            "Extracting vocabulary...\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000 ...\n",
            "Read 2592092 words (30336 unique) from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000 ...\n",
            "Read 2057480 words (42782 unique) from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000 ...\n",
            "Read 2592092 words (30336 unique) from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000 ...\n",
            "Read 2057480 words (59437 unique) from text file.\n",
            "EN vocab in: /content/UnsupervisedMT/NMT/data/mono/vocab.en.60000\n",
            "zu vocab in: /content/UnsupervisedMT/NMT/data/mono/vocab.zu.60000\n",
            "Full vocab in: /content/UnsupervisedMT/NMT/data/mono/vocab.en-zu.60000\n",
            "Binarizing data...\n",
            "INFO - 11/21/18 19:46:11 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000.pth ...\n",
            "INFO - 11/21/18 19:46:14 - 0:00:03 - 2592092 words (59451 unique) in 100000 sentences.\n",
            "INFO - 11/21/18 19:46:14 - 0:00:03 - 0 unknown word.\n",
            "INFO - 11/21/18 19:46:14 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000.pth ...\n",
            "INFO - 11/21/18 19:46:17 - 0:00:02 - 2057480 words (59451 unique) in 100000 sentences.\n",
            "INFO - 11/21/18 19:46:17 - 0:00:02 - 0 unknown word.\n",
            "EN binarized data in: /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000.pth\n",
            "zu binarized data in: /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000.pth\n",
            "Downloading parallel data...\n",
            "--2018-11-21 19:46:17--  https://github.com/LauraMartinus/ukuxhumana/raw/master/clean/en_zu/enzu_parallel.dev.en\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.dev.en [following]\n",
            "--2018-11-21 19:46:17--  https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.dev.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 586751 (573K) [text/plain]\n",
            "Saving to: ‘enzu_parallel.dev.en’\n",
            "\n",
            "enzu_parallel.dev.e 100%[===================>] 573.00K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-11-21 19:46:17 (5.45 MB/s) - ‘enzu_parallel.dev.en’ saved [586751/586751]\n",
            "\n",
            "--2018-11-21 19:46:17--  https://github.com/LauraMartinus/ukuxhumana/raw/master/clean/en_zu/enzu_parallel.dev.zu\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.dev.zu [following]\n",
            "--2018-11-21 19:46:17--  https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.dev.zu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 623825 (609K) [text/plain]\n",
            "Saving to: ‘enzu_parallel.dev.zu’\n",
            "\n",
            "enzu_parallel.dev.z 100%[===================>] 609.20K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-11-21 19:46:18 (5.54 MB/s) - ‘enzu_parallel.dev.zu’ saved [623825/623825]\n",
            "\n",
            "--2018-11-21 19:46:18--  https://github.com/LauraMartinus/ukuxhumana/raw/master/clean/en_zu/enzu_parallel.test.en\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.test.en [following]\n",
            "--2018-11-21 19:46:18--  https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.test.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 377479 (369K) [text/plain]\n",
            "Saving to: ‘enzu_parallel.test.en’\n",
            "\n",
            "enzu_parallel.test. 100%[===================>] 368.63K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2018-11-21 19:46:18 (6.62 MB/s) - ‘enzu_parallel.test.en’ saved [377479/377479]\n",
            "\n",
            "--2018-11-21 19:46:18--  https://github.com/LauraMartinus/ukuxhumana/raw/master/clean/en_zu/enzu_parallel.test.zu\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.test.zu [following]\n",
            "--2018-11-21 19:46:18--  https://raw.githubusercontent.com/LauraMartinus/ukuxhumana/master/clean/en_zu/enzu_parallel.test.zu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 403406 (394K) [text/plain]\n",
            "Saving to: ‘enzu_parallel.test.zu’\n",
            "\n",
            "enzu_parallel.test. 100%[===================>] 393.95K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2018-11-21 19:46:19 (4.70 MB/s) - ‘enzu_parallel.test.zu’ saved [403406/403406]\n",
            "\n",
            "Extracting parallel data...\n",
            "Tokenizing valid and test data...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 48\n",
            "Tokenizer Version 1.1\n",
            "Language: zu\n",
            "Number of threads: 48\n",
            "WARNING: No known abbreviations for language 'zu', attempting fall-back to English version...\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 48\n",
            "Tokenizer Version 1.1\n",
            "Language: zu\n",
            "Number of threads: 48\n",
            "WARNING: No known abbreviations for language 'zu', attempting fall-back to English version...\n",
            "Applying BPE to valid and test files...\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/vocab.en.60000 ...\n",
            "Read 2592092 words (30336 unique) from vocabulary file.\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.en ...\n",
            "Read 104371 words (8205 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.en ...\n",
            "Modified 104371 words from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/vocab.zu.60000 ...\n",
            "Read 2057480 words (42782 unique) from vocabulary file.\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.zu ...\n",
            "Read 73915 words (20623 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.zu ...\n",
            "Modified 73915 words from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/vocab.en.60000 ...\n",
            "Read 2592092 words (30336 unique) from vocabulary file.\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.en ...\n",
            "Read 65604 words (6694 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.en ...\n",
            "Modified 65604 words from text file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/mono/vocab.zu.60000 ...\n",
            "Read 2057480 words (42782 unique) from vocabulary file.\n",
            "Loading codes from /content/UnsupervisedMT/NMT/data/mono/bpe_codes ...\n",
            "Read 60000 codes from the codes file.\n",
            "Loading vocabulary from /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.zu ...\n",
            "Read 48512 words (14309 unique) from text file.\n",
            "Applying BPE to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.zu ...\n",
            "Modified 48512 words from text file.\n",
            "Binarizing data...\n",
            "INFO - 11/21/18 19:46:22 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.en.60000.pth ...\n",
            "INFO - 11/21/18 19:46:23 - 0:00:00 - 113806 words (59451 unique) in 5019 sentences.\n",
            "INFO - 11/21/18 19:46:23 - 0:00:00 - 0 unknown word.\n",
            "INFO - 11/21/18 19:46:23 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.zu.60000.pth ...\n",
            "INFO - 11/21/18 19:46:24 - 0:00:00 - 113824 words (59451 unique) in 5019 sentences.\n",
            "INFO - 11/21/18 19:46:24 - 0:00:00 - 0 unknown word.\n",
            "INFO - 11/21/18 19:46:24 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.en.60000.pth ...\n",
            "INFO - 11/21/18 19:46:24 - 0:00:00 - 73804 words (59451 unique) in 3000 sentences.\n",
            "INFO - 11/21/18 19:46:24 - 0:00:00 - 0 unknown word.\n",
            "INFO - 11/21/18 19:46:25 - 0:00:00 - Read 59451 words from the vocabulary file.\n",
            "\n",
            "Saving the data to /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.zu.60000.pth ...\n",
            "INFO - 11/21/18 19:46:25 - 0:00:00 - 77509 words (59451 unique) in 3000 sentences.\n",
            "INFO - 11/21/18 19:46:25 - 0:00:00 - 0 unknown word.\n",
            "\n",
            "===== Data summary\n",
            "Monolingual training data:\n",
            "    en: /content/UnsupervisedMT/NMT/data/mono/mono.en.tok.60000.pth\n",
            "    zu: /content/UnsupervisedMT/NMT/data/mono/mono.zu.tok.60000.pth\n",
            "Parallel validation data:\n",
            "    en: /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.en.60000.pth\n",
            "    zu: /content/UnsupervisedMT/NMT/data/para/enzu_parallel.dev.zu.60000.pth\n",
            "Parallel test data:\n",
            "    en: /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.en.60000.pth\n",
            "    zu: /content/UnsupervisedMT/NMT/data/para/enzu_parallel.test.zu.60000.pth\n",
            "\n",
            "Concatenating source and target monolingual data...\n",
            "Concatenated data in: /content/UnsupervisedMT/NMT/data/mono/all.en-zu.60000\n",
            "Training fastText on /content/UnsupervisedMT/NMT/data/mono/all.en-zu.60000...\n",
            "Read 4M words\n",
            "Number of words:  59438\n",
            "Number of labels: 0\n",
            "tcmalloc: large alloc 4217733120 bytes == 0x564acb6ee000 @  0x7f5a57d7f887 0x564ac2bdff73 0x564ac2bf97b0 0x564ac2bfff7c 0x564ac2bca0f7 0x7f5a56e1cb97 0x564ac2bca3ba\n",
            "^C\n",
            "cp: target 'Drive/NMT/data/*' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mbeK3jJGz6K9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you want to save to drive\n",
        "!cp -r /content/UnsupervisedMT/NMT/data/* \"/content/gdrive/My Drive/NMT/data/\"\n",
        "\n",
        "# If you want to pull in from drive\n",
        "#!cp -r \"/content/gdrive/My Drive/NMT/data/*\" /content/UnsupervisedMT/NMT/data/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fpiBbXnfHaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save weights to drive\n",
        "file_path='/content/UnsupervisedMT/NMT/data/mono/all.en-zu.60000.vec'\n",
        "destination_path='/content/gdrive/My Drive/NMT/all.en-zu.60000.vec'\n",
        "with open(destination_path, 'w') as f:\n",
        "  with open(file_path,'r') as to_move:\n",
        "    f.write(to_move.read())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mu_tE7NusVjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8353
        },
        "outputId": "7226add5-43d2-437a-e60d-a2e8f88884e5"
      },
      "cell_type": "code",
      "source": [
        "# Run the unsupervised nmt\n",
        "!cd UnsupervisedMT/NMT; python -u main.py --exp_name enzu     --transformer True  --n_enc_layers 4 --n_dec_layers 4      --share_enc 3  --share_dec 3  --share_lang_emb True  --share_output_emb True     --langs \"en,zu\" --n_mono -1  --mono_dataset \"en:./data/mono/mono.en.tok.60000.pth,,;zu:./data/mono/mono.zu.tok.60000.pth,,\"  --para_dataset \"en-zu:,./data/para/enzu_parallel.dev.en.60000.pth,./data/para/enzu_parallel.dev.zu.60000.pth\"     --mono_directions \"en,zu\"  --word_shuffle 3 --word_dropout 0.1 --word_blank 0.2     --pivo_directions \"en-zu-en,zu-en-zu\"     --pretrained_emb \"/content/gdrive/My Drive/NMT/all.en-zu.60000.vec\"  --pretrained_out True     --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd 1     --otf_num_processes 30   --otf_sync_params_every 1000     --enc_optimizer adam,lr=0.0001     --group_by_size True     --batch_size 32     --epoch_size 500000 --stopping_criterion bleu_en_zu_valid,10     --freeze_enc_emb False --freeze_dec_emb False     --dump_path \"/content/gdrive/My Drive/NMT\"        \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 11/21/18 19:53:14 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 11/21/18 19:53:14 - 0:00:00 - attention: True\n",
            "                                     attention_dropout: 0\n",
            "                                     back_dataset: {}\n",
            "                                     back_directions: []\n",
            "                                     batch_size: 32\n",
            "                                     beam_size: 0\n",
            "                                     clip_grad_norm: 5\n",
            "                                     command: python main.py --exp_name 'enzu' --transformer 'True' --n_enc_layers '4' --n_dec_layers '4' --share_enc '3' --share_dec '3' --share_lang_emb 'True' --share_output_emb 'True' --langs 'en,zu' --n_mono '-1' --mono_dataset 'en:./data/mono/mono.en.tok.60000.pth,,;zu:./data/mono/mono.zu.tok.60000.pth,,' --para_dataset 'en-zu:,./data/para/enzu_parallel.dev.en.60000.pth,./data/para/enzu_parallel.dev.zu.60000.pth' --mono_directions 'en,zu' --word_shuffle '3' --word_dropout '0.1' --word_blank '0.2' --pivo_directions 'en-zu-en,zu-en-zu' --pretrained_emb '/content/gdrive/My Drive/NMT/all.en-zu.60000.vec' --pretrained_out 'True' --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd '1' --otf_num_processes '30' --otf_sync_params_every '1000' --enc_optimizer 'adam,lr=0.0001' --group_by_size 'True' --batch_size '32' --epoch_size '500000' --stopping_criterion 'bleu_en_zu_valid,10' --freeze_enc_emb 'False' --freeze_dec_emb 'False' --dump_path '/content/gdrive/My Drive/NMT' --exp_id \"dpthjnvw48\"\n",
            "                                     dec_optimizer: enc_optimizer\n",
            "                                     decoder_attention_heads: 8\n",
            "                                     decoder_normalize_before: False\n",
            "                                     dis_clip: 0\n",
            "                                     dis_dropout: 0\n",
            "                                     dis_hidden_dim: 128\n",
            "                                     dis_input_proj: True\n",
            "                                     dis_layers: 3\n",
            "                                     dis_optimizer: rmsprop,lr=0.0005\n",
            "                                     dis_smooth: 0\n",
            "                                     dropout: 0\n",
            "                                     dump_path: /content/gdrive/My Drive/NMT/enzu/dpthjnvw48\n",
            "                                     emb_dim: 512\n",
            "                                     enc_optimizer: adam,lr=0.0001\n",
            "                                     encoder_attention_heads: 8\n",
            "                                     encoder_normalize_before: False\n",
            "                                     epoch_size: 500000\n",
            "                                     eval_only: False\n",
            "                                     exp_id: dpthjnvw48\n",
            "                                     exp_name: enzu\n",
            "                                     freeze_dec_emb: False\n",
            "                                     freeze_enc_emb: False\n",
            "                                     group_by_size: True\n",
            "                                     hidden_dim: 512\n",
            "                                     id2lang: {0: 'en', 1: 'zu'}\n",
            "                                     label_smoothing: 0\n",
            "                                     lambda_dis: 0\n",
            "                                     lambda_lm: 0\n",
            "                                     lambda_xe_back: 0\n",
            "                                     lambda_xe_mono: 0:1,100000:0.1,300000:0\n",
            "                                     lambda_xe_otfa: 0\n",
            "                                     lambda_xe_otfd: 1\n",
            "                                     lambda_xe_para: 0\n",
            "                                     lang2id: {'en': 0, 'zu': 1}\n",
            "                                     langs: ['en', 'zu']\n",
            "                                     length_penalty: 1.0\n",
            "                                     lm_after: 0\n",
            "                                     lm_before: 0\n",
            "                                     lm_share_dec: 0\n",
            "                                     lm_share_emb: False\n",
            "                                     lm_share_enc: 0\n",
            "                                     lm_share_proj: False\n",
            "                                     lstm_proj: False\n",
            "                                     max_epoch: 100000\n",
            "                                     max_len: 175\n",
            "                                     max_vocab: -1\n",
            "                                     mono_dataset: {'en': ('./data/mono/mono.en.tok.60000.pth', '', ''), 'zu': ('./data/mono/mono.zu.tok.60000.pth', '', '')}\n",
            "                                     mono_directions: ['en', 'zu']\n",
            "                                     n_back: 0\n",
            "                                     n_dec_layers: 4\n",
            "                                     n_dis: 0\n",
            "                                     n_enc_layers: 4\n",
            "                                     n_langs: 2\n",
            "                                     n_mono: -1\n",
            "                                     n_para: 0\n",
            "                                     otf_backprop_temperature: -1\n",
            "                                     otf_num_processes: 30\n",
            "                                     otf_sample: -1\n",
            "                                     otf_sync_params_every: 1000\n",
            "                                     otf_update_dec: True\n",
            "                                     otf_update_enc: True\n",
            "                                     para_dataset: {('en', 'zu'): ('', './data/para/enzu_parallel.dev.en.60000.pth', './data/para/enzu_parallel.dev.zu.60000.pth')}\n",
            "                                     para_directions: []\n",
            "                                     pivo_directions: [('en', 'zu', 'en'), ('zu', 'en', 'zu')]\n",
            "                                     pretrained_emb: /content/gdrive/My Drive/NMT/all.en-zu.60000.vec\n",
            "                                     pretrained_out: True\n",
            "                                     reload_dec: False\n",
            "                                     reload_dis: False\n",
            "                                     reload_enc: False\n",
            "                                     reload_model: \n",
            "                                     relu_dropout: 0\n",
            "                                     save_periodic: False\n",
            "                                     seed: -1\n",
            "                                     share_dec: 3\n",
            "                                     share_decpro_emb: False\n",
            "                                     share_enc: 3\n",
            "                                     share_encdec_emb: False\n",
            "                                     share_lang_emb: True\n",
            "                                     share_lstm_proj: False\n",
            "                                     share_output_emb: True\n",
            "                                     stopping_criterion: bleu_en_zu_valid,10\n",
            "                                     transformer: True\n",
            "                                     transformer_ffn_emb_dim: 2048\n",
            "                                     vocab: {}\n",
            "                                     vocab_min_count: 0\n",
            "                                     word_blank: 0.2\n",
            "                                     word_dropout: 0.1\n",
            "                                     word_shuffle: 3.0\n",
            "INFO - 11/21/18 19:53:14 - 0:00:00 - The experiment will be stored in /content/gdrive/My Drive/NMT/enzu/dpthjnvw48\n",
            "                                     \n",
            "INFO - 11/21/18 19:53:14 - 0:00:00 - Running command: python main.py --exp_name 'enzu' --transformer 'True' --n_enc_layers '4' --n_dec_layers '4' --share_enc '3' --share_dec '3' --share_lang_emb 'True' --share_output_emb 'True' --langs 'en,zu' --n_mono '-1' --mono_dataset 'en:./data/mono/mono.en.tok.60000.pth,,;zu:./data/mono/mono.zu.tok.60000.pth,,' --para_dataset 'en-zu:,./data/para/enzu_parallel.dev.en.60000.pth,./data/para/enzu_parallel.dev.zu.60000.pth' --mono_directions 'en,zu' --word_shuffle '3' --word_dropout '0.1' --word_blank '0.2' --pivo_directions 'en-zu-en,zu-en-zu' --pretrained_emb '/content/gdrive/My Drive/NMT/all.en-zu.60000.vec' --pretrained_out 'True' --lambda_xe_mono '0:1,100000:0.1,300000:0' --lambda_xe_otfd '1' --otf_num_processes '30' --otf_sync_params_every '1000' --enc_optimizer 'adam,lr=0.0001' --group_by_size 'True' --batch_size '32' --epoch_size '500000' --stopping_criterion 'bleu_en_zu_valid,10' --freeze_enc_emb 'False' --freeze_dec_emb 'False' --dump_path '/content/gdrive/My Drive/NMT' --exp_id \"dpthjnvw48\"\n",
            "                                     \n",
            "INFO - 11/21/18 19:53:14 - 0:00:00 - ============ Parallel data (en - zu)\n",
            "INFO - 11/21/18 19:53:14 - 0:00:00 - Loading data from ./data/para/enzu_parallel.dev.en.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - 113806 words (59451 unique) in 5019 sentences. 0 unknown words (0 unique).\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Reloading data loaded from ./data/para/enzu_parallel.dev.en.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Removed 0 empty sentences.\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Removed 0 too long sentences.\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Loading data from ./data/para/enzu_parallel.dev.zu.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - 113824 words (59451 unique) in 5019 sentences. 0 unknown words (0 unique).\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Reloading data loaded from ./data/para/enzu_parallel.dev.zu.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Removed 0 empty sentences.\n",
            "\n",
            "\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - ============ Monolingual data (en)\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - Loading data from ./data/mono/mono.en.tok.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:00 - 2592092 words (59451 unique) in 100000 sentences. 0 unknown words (0 unique).\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Removed 0 empty sentences.\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Removed 85 too long sentences.\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - ============ Monolingual data (zu)\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Loading data from ./data/mono/mono.zu.tok.60000.pth ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - 2057480 words (59451 unique) in 100000 sentences. 0 unknown words (0 unique).\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Removed 0 empty sentences.\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Removed 0 too long sentences.\n",
            "\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - ============ Data summary\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Parallel data      - valid -   en ->   zu:      5019\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Parallel data      -  test -   en ->   zu:      5019\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   - train -           en:     99915\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   - valid -           en:         0\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   -  test -           en:         0\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   - train -           zu:    100000\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   - valid -           zu:         0\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Monolingual data   -  test -           zu:         0\n",
            "\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - ============ Building transformer attention model - Encoder ...\n",
            "INFO - 11/21/18 19:53:15 - 0:00:01 - Sharing encoder input embeddings\n",
            "INFO - 11/21/18 19:53:16 - 0:00:02 - Sharing encoder transformer parameters for layer 1\n",
            "INFO - 11/21/18 19:53:16 - 0:00:02 - Sharing encoder transformer parameters for layer 2\n",
            "INFO - 11/21/18 19:53:16 - 0:00:02 - Sharing encoder transformer parameters for layer 3\n",
            "\n",
            "INFO - 11/21/18 19:53:16 - 0:00:02 - ============ Building transformer attention model - Decoder ...\n",
            "INFO - 11/21/18 19:53:16 - 0:00:02 - Sharing decoder input embeddings\n",
            "INFO - 11/21/18 19:53:17 - 0:00:03 - Sharing decoder transformer parameters for layer 0\n",
            "INFO - 11/21/18 19:53:17 - 0:00:03 - Sharing decoder transformer parameters for layer 1\n",
            "INFO - 11/21/18 19:53:17 - 0:00:03 - Sharing decoder transformer parameters for layer 2\n",
            "INFO - 11/21/18 19:53:18 - 0:00:04 - Sharing decoder projection matrices\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "INFO - 11/21/18 19:53:24 - 0:00:09 - Reloading embeddings from /content/gdrive/My Drive/NMT/all.en-zu.60000.vec ...\n",
            "INFO - 11/21/18 19:53:31 - 0:00:17 - Reloaded 59438 embeddings.\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Initialized 59438 / 59451 word embeddings for \"en\" (including 0 after lowercasing).\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Initialized 59438 / 59451 word embeddings for \"zu\" (including 0 after lowercasing).\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - ============ Model summary\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Number of enc+dec parameters: 128158267\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Encoder: TransformerEncoder(\n",
            "                                       (embeddings): ModuleList(\n",
            "                                         (0): Embedding(59451, 512, padding_idx=2)\n",
            "                                         (1): Embedding(59451, 512, padding_idx=2)\n",
            "                                       )\n",
            "                                       (embed_positions): SinusoidalPositionalEmbedding()\n",
            "                                       (layers): ModuleList(\n",
            "                                         (0): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (1): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (2): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (3): ModuleList(\n",
            "                                           (0): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerEncoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                       )\n",
            "                                     )\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Decoder: TransformerDecoder(\n",
            "                                       (embeddings): ModuleList(\n",
            "                                         (0): Embedding(59451, 512, padding_idx=2)\n",
            "                                         (1): Embedding(59451, 512, padding_idx=2)\n",
            "                                       )\n",
            "                                       (embed_positions): SinusoidalPositionalEmbedding()\n",
            "                                       (layers): ModuleList(\n",
            "                                         (0): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (1): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (2): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                         (3): ModuleList(\n",
            "                                           (0): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                           (1): TransformerDecoderLayer(\n",
            "                                             (self_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (encoder_attn): MultiheadAttention(\n",
            "                                               (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "                                             )\n",
            "                                             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                                             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                                             (layer_norms): ModuleList(\n",
            "                                               (0): LayerNorm()\n",
            "                                               (1): LayerNorm()\n",
            "                                               (2): LayerNorm()\n",
            "                                             )\n",
            "                                           )\n",
            "                                         )\n",
            "                                       )\n",
            "                                       (proj): ModuleList(\n",
            "                                         (0): Linear(in_features=512, out_features=59451, bias=True)\n",
            "                                         (1): Linear(in_features=512, out_features=59451, bias=True)\n",
            "                                       )\n",
            "                                       (loss_fn): ModuleList(\n",
            "                                         (0): CrossEntropyLoss()\n",
            "                                         (1): CrossEntropyLoss()\n",
            "                                       )\n",
            "                                     )\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - Discriminator: None\n",
            "INFO - 11/21/18 19:53:44 - 0:00:30 - LM: None\n",
            "\n",
            "INFO - 11/21/18 19:53:52 - 0:00:38 - Starting subprocesses for OTF generation ...\n",
            "INFO - 11/21/18 19:53:52 - 0:00:38 - Stopping criterion: bleu_en_zu_valid,10\n",
            "INFO - 11/21/18 19:53:54 - 0:00:40 - Test: Parameters are shared correctly.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "INFO - 11/21/18 19:55:11 - 0:01:57 - ====================== Starting epoch 0 ... ======================\n",
            "INFO - 11/21/18 19:55:11 - 0:01:57 - Creating new training encdec,en iterator ...\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "INFO - 11/21/18 19:55:12 - 0:01:58 - Creating new training encdec,zu iterator ...\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 313, in <module>\n",
            "    trainer.otf_sync_params()\n",
            "  File \"/content/UnsupervisedMT/NMT/src/trainer.py\", line 537, in otf_sync_params\n",
            "    decoder_params=decoder_params)\n",
            "  File \"/content/UnsupervisedMT/NMT/src/multiprocessing_event_loop.py\", line 82, in call_async\n",
            "    self.input_pipes[rank].send((result_type, action, kwargs))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}